{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaee5001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/LoeffelRL/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/LoeffelRL/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/workspace/LoeffelRL/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 17:10:21 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-29 17:10:21 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 17:10:22,276\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict\n",
    "import csv\n",
    "import random\n",
    "import art\n",
    "from art.local import LocalBackend\n",
    "import weave\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "import re\n",
    "import Levenshtein\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e125eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in transforming standard German into German Spoon Language or LÃ¶ffelsprache.\n",
    "Given a German sentence, you will transform it into German Spoon Language.\n",
    "Follow these strict rules:\n",
    "\n",
    "Let x be any of the following vowels or vowel pairs:\n",
    "{ei, ie, au, eu, Ã¤u, a, e, i, o, u}\n",
    "For each occurrence of x (here a variable), replace it with xlewx.\n",
    "Example: a â†’ alewa, ei â†’ eilewei\n",
    "Always match vowel pairs first, before checking for single vowels.\n",
    "After a replacement, continue from the end of the replaced text â€” do not reprocess inside the result.\n",
    "Preserve casing:\n",
    "If the original x begins with an uppercase letter, only the first letter of the xlewx replacement is uppercase.\n",
    "Example: A â†’ Alewa, Ei â†’ Eilewei, Au â†’ Aulewau\n",
    "Example words:\n",
    "Hallo -> Halewallolewo\n",
    "Eier -> Eileweielewer\n",
    "Do not apply transformations recursively.\n",
    "Return only the converted sentence, wrapped in <spoon> ... </spoon> tags.\n",
    "Do not explain your transformation.\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "class SentencePair(BaseModel):\n",
    "    german: str\n",
    "    spoon: str\n",
    "    \n",
    "def load_data(file_path: str) -> list[SentencePair]:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        return [SentencePair(german=row[0], spoon=row[1]) for row in reader]\n",
    "\n",
    "def draw_sample(data: list[SentencePair]) -> SentencePair:\n",
    "    return random.choice(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4e1d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError,))\n",
    "async def rollout(model: art.Model, pair: SentencePair) -> art.Trajectory:\n",
    "    trajectory = art.Trajectory(\n",
    "        messages_and_choices=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT,\n",
    "            }\n",
    "        ],\n",
    "        metadata={\n",
    "            \"notebook-id\": \"SpoonRL\",\n",
    "        },\n",
    "        reward=0,\n",
    "    )\n",
    "    trajectory.messages_and_choices.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": pair.german,\n",
    "    })\n",
    "    messages = trajectory.messages()\n",
    "    try:\n",
    "        client = model.openai_client()\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            model=model.get_model_name(),\n",
    "            messages=messages,\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "    except openai.LengthFinishReasonError as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(\"Caught exception generating chat comopletion\")\n",
    "        print(e)\n",
    "        global failing_trajectory\n",
    "        failing_trajectory = trajectory\n",
    "        raise e\n",
    "    \n",
    "    choice = chat_completion.choices[0]\n",
    "    content = choice.message.content\n",
    "    \n",
    "    format_reward = 0\n",
    "    match = re.search(r\"<spoon>(.*?)</spoon>\", content, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "        format_reward = 1\n",
    "    else:\n",
    "        match = content\n",
    "    dist = Levenshtein.distance(match, pair.spoon)\n",
    "    max_len = max(len(match), len(pair.spoon), 1)\n",
    "    spoon_reward = 1.0 - dist / max_len  \n",
    "    \n",
    "    reward = spoon_reward * 0.8 + format_reward * 0.2\n",
    "    trajectory.reward = reward\n",
    "    return trajectory    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a036350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"../data/german_spoon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697c6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "backend = LocalBackend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c662bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/LoeffelRL/.venv/lib/python3.10/site-packages/art/__init__.py:11: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/LoeffelRL/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/workspace/LoeffelRL/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 17:11:09 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-29 17:11:09 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.256 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading wambosec/Qwen2.5-7B-Instruct-spoon-language-SFT with actual GPU utilization = 78.55%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.26 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 368.\n",
      "Unsloth: vLLM's KV Cache can use up to 48.03 GB. Also swap space = 6 GB.\n",
      "INFO 07-29 17:11:15 [config.py:2968] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 07-29 17:11:24 [config.py:717] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 07-29 17:11:24 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='wambosec/Qwen2.5-7B-Instruct-spoon-language-SFT', speculative_config=None, tokenizer='wambosec/Qwen2.5-7B-Instruct-spoon-language-SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=wambosec/Qwen2.5-7B-Instruct-spoon-language-SFT, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":368}, use_cached_outputs=False, \n",
      "INFO 07-29 17:11:25 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 07-29 17:11:26 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 07-29 17:11:26 [model_runner.py:1108] Starting to load model wambosec/Qwen2.5-7B-Instruct-spoon-language-SFT...\n",
      "INFO 07-29 17:11:26 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 07-29 17:11:26 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:01<00:09,  1.54s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:02<00:07,  1.47s/it]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:04<00:06,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:06<00:04,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:07<00:02,  1.45s/it]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:08<00:01,  1.32s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:09<00:00,  1.06s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:09<00:00,  1.29s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 17:11:36 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 07-29 17:11:36 [model_runner.py:1140] Model loading took 5.2451 GiB and 9.983583 seconds\n",
      "INFO 07-29 17:12:03 [worker.py:287] Memory profiling takes 26.02 seconds\n",
      "INFO 07-29 17:12:03 [worker.py:287] the current vLLM instance can use total_gpu_memory (79.26GiB) x gpu_memory_utilization (0.79) = 62.25GiB\n",
      "INFO 07-29 17:12:03 [worker.py:287] model weights take 5.25GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 8.29GiB; the rest of the memory reserved for KV Cache is 48.62GiB.\n",
      "INFO 07-29 17:12:03 [executor_base.py:112] # cuda blocks: 56904, # CPU blocks: 7021\n",
      "INFO 07-29 17:12:03 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 27.79x\n",
      "INFO 07-29 17:12:09 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [01:17<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 17:13:27 [model_runner.py:1592] Graph capturing finished in 78 secs, took 1.58 GiB\n",
      "INFO 07-29 17:13:27 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 110.60 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'k_norm', 'pre_feedforward_layernorm', 'q_norm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'k_norm', 'pre_feedforward_layernorm', 'q_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = art.TrainableModel(\n",
    "    name=\"001-german-spoon\",\n",
    "    project=\"SpoonRL\",\n",
    "    base_model=\"wambosec/Qwen2.5-7B-Instruct-spoon-language-SFT\",\n",
    ")\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66817225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
